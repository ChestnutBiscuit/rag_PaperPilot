# ================================
# rag.py - å¢å¼ºRAGç³»ç»Ÿä¸»å…¥å£ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰
# ================================

import logging
import json
import numpy as np
import os
import time
import pickle
from typing import List, Dict, Any, Optional
from pathlib import Path
import sys

# å¯¼å…¥æ‰€æœ‰æ¨¡å—
from config import RAGConfig
from chunking.extractor import StructuredPDFExtractor 
#from chunking.extractor import PDFExtractor
from chunking.embedder import ChunkVectorizer
from vector_store.build_index import VectorIndexBuilder
from vector_store.retriever import DocumentRetriever
from llm.llm_client import LLMClient


class IncrementalVectorUpdater:
    """å¢é‡å‘é‡åº“æ›´æ–°å™¨"""
    
    def __init__(self, rag_config: RAGConfig):
        self.config = rag_config
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.pdf_extractor = StructuredPDFExtractor(rag_config)
        self.vectorizer = ChunkVectorizer(rag_config)
        self.index_builder = VectorIndexBuilder(rag_config)
    
    def load_existing_vectors(self) -> tuple[np.ndarray, List[Dict], bool]:
        """åŠ è½½ç°æœ‰çš„å‘é‡å’Œchunks"""
        try:
            embeddings_file = os.path.join(self.config.embedding.OUTPUT_DIR, "embeddings.npy")
            metadata_file = os.path.join(self.config.embedding.OUTPUT_DIR, "metadata.json")
            
            if os.path.exists(embeddings_file) and os.path.exists(metadata_file):
                embeddings = np.load(embeddings_file)
                
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                chunks = metadata.get('chunks', [])
                self.logger.info(f"æˆåŠŸåŠ è½½ç°æœ‰å‘é‡: {embeddings.shape}, æ–‡æ¡£å—: {len(chunks)}")
                return embeddings, chunks, True
            else:
                self.logger.info("æœªæ‰¾åˆ°ç°æœ‰å‘é‡æ•°æ®ï¼Œå°†åˆ›å»ºæ–°çš„å‘é‡åº“")
                return np.array([]), [], False
                
        except Exception as e:
            self.logger.error(f"åŠ è½½ç°æœ‰å‘é‡å¤±è´¥: {e}")
            return np.array([]), [], False
    
    def get_processed_files(self, chunks: List[Dict]) -> set:
        """è·å–å·²å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨"""
        processed_files = set()
        for chunk in chunks:
            if 'source_file' in chunk.get('metadata', {}):
                processed_files.add(chunk['metadata']['source_file'])
        return processed_files
    
    def find_new_pdf_files(self, pdf_dir: str = None, processed_files: set = None) -> List[str]:
        """æ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–°PDFæ–‡ä»¶"""
        pdf_dir = pdf_dir or self.config.PDF_DIR
        processed_files = processed_files or set()
        
        all_pdf_files = list(Path(pdf_dir).glob("*.json"))
        new_files = []
        
        for pdf_path in all_pdf_files:
            filename = pdf_path.name
            if filename not in processed_files:
                new_files.append(str(pdf_path))
        
        self.logger.info(f"å‘ç° {len(new_files)} ä¸ªæ–°çš„PDFæ–‡ä»¶éœ€è¦å¤„ç†")
        return new_files
    
    def process_new_pdfs(self, new_pdf_files: List[str]) -> List[Dict[str, Any]]:
        """å¤„ç†æ–°çš„jsonæ–‡ä»¶"""
        all_new_chunks = []
        
        for i, pdf_path in enumerate(new_pdf_files, 1):
            self.logger.info(f"å¤„ç†æ–°æ–‡ä»¶ [{i}/{len(new_pdf_files)}]: {Path(pdf_path).name}")
            
            try:
                chunks = self.pdf_extractor.process_structured_pdf_file(pdf_path)
                
                # æ·»åŠ æºæ–‡ä»¶ä¿¡æ¯
                filename = Path(pdf_path).name
                for chunk in chunks:
                    chunk["metadata"]["source_file"] = filename
                
                all_new_chunks.extend(chunks)
                self.logger.info(f"æˆåŠŸæå– {len(chunks)} ä¸ªæ–‡æ¡£å—")
                
            except Exception as e:
                self.logger.error(f"å¤„ç†PDFæ–‡ä»¶å¤±è´¥ {pdf_path}: {e}")
                continue
        
        return all_new_chunks
    
    def vectorize_new_chunks(self, new_chunks: List[Dict[str, Any]]) -> np.ndarray:
        """å¯¹æ–°æ–‡æ¡£å—è¿›è¡Œå‘é‡åŒ–"""
        if not new_chunks:
            return np.array([])
        
        self.logger.info(f"å¼€å§‹å‘é‡åŒ– {len(new_chunks)} ä¸ªæ–°æ–‡æ¡£å—...")
        
        # åŠ è½½å‘é‡åŒ–æ¨¡å‹
        self.vectorizer.load_model()
        
        # è®¾ç½®æ–°çš„chunks
        self.vectorizer.load_chunks_from_list(new_chunks)
        
        # è¿›è¡Œå‘é‡åŒ–
        if self.vectorizer.vectorize_chunks():
            self.logger.info(f"æ–°æ–‡æ¡£å—å‘é‡åŒ–å®Œæˆ: {self.vectorizer.embeddings.shape}")
            return self.vectorizer.embeddings
        else:
            raise RuntimeError("æ–°æ–‡æ¡£å—å‘é‡åŒ–å¤±è´¥")
    
    def merge_vectors(self, existing_embeddings: np.ndarray, new_embeddings: np.ndarray) -> np.ndarray:
        """åˆå¹¶ç°æœ‰å‘é‡å’Œæ–°å‘é‡"""
        if existing_embeddings.size == 0:
            return new_embeddings
        
        if new_embeddings.size == 0:
            return existing_embeddings
        
        # æ£€æŸ¥ç»´åº¦æ˜¯å¦åŒ¹é…
        if existing_embeddings.shape[1] != new_embeddings.shape[1]:
            raise ValueError(f"å‘é‡ç»´åº¦ä¸åŒ¹é…: ç°æœ‰ {existing_embeddings.shape[1]}, æ–°å¢ {new_embeddings.shape[1]}")
        
        merged_embeddings = np.vstack([existing_embeddings, new_embeddings])
        self.logger.info(f"å‘é‡åˆå¹¶å®Œæˆ: {existing_embeddings.shape} + {new_embeddings.shape} = {merged_embeddings.shape}")
        return merged_embeddings
    
    def merge_chunks(self, existing_chunks: List[Dict], new_chunks: List[Dict]) -> List[Dict]:
        """åˆå¹¶ç°æœ‰chunkså’Œæ–°chunks"""
        # ä¸ºæ–°chunksæ·»åŠ å…¨å±€ç´¢å¼•
        start_index = len(existing_chunks)
        for i, chunk in enumerate(new_chunks):
            chunk["metadata"]["global_index"] = start_index + i
        
        merged_chunks = existing_chunks + new_chunks
        self.logger.info(f"æ–‡æ¡£å—åˆå¹¶å®Œæˆ: {len(existing_chunks)} + {len(new_chunks)} = {len(merged_chunks)}")
        return merged_chunks
    
    def update_vector_store(self, merged_embeddings: np.ndarray, merged_chunks: List[Dict]):
        """æ›´æ–°å‘é‡å­˜å‚¨"""
        try:
            # ä¿å­˜åˆå¹¶åçš„å‘é‡
            embeddings_file = os.path.join(self.config.embedding.OUTPUT_DIR, "embeddings.npy")
            metadata_file = os.path.join(self.config.embedding.OUTPUT_DIR, "metadata.json")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(self.config.embedding.OUTPUT_DIR, exist_ok=True)
            
            # ä¿å­˜å‘é‡
            np.save(embeddings_file, merged_embeddings)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                "chunks": merged_chunks,
                "model_name": self.config.embedding.EMBEDDING_MODEL,
                "embedding_dim": merged_embeddings.shape[1],
                "num_chunks": len(merged_chunks),
                "normalized": self.config.embedding.NORMALIZE_EMBEDDINGS,
                "last_updated": time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"å‘é‡æ•°æ®å·²ä¿å­˜åˆ° {embeddings_file} å’Œ {metadata_file}")
            
            # é‡å»ºFAISSç´¢å¼•
            self.rebuild_faiss_index(merged_embeddings, merged_chunks)
            
        except Exception as e:
            self.logger.error(f"æ›´æ–°å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            raise
    
    def rebuild_faiss_index(self, embeddings: np.ndarray, chunks: List[Dict]):
        """é‡å»ºFAISSç´¢å¼•"""
        try:
            self.logger.info("é‡å»ºFAISSç´¢å¼•...")
            
            # ä½¿ç”¨ç°æœ‰çš„index_builderé‡å»ºç´¢å¼•
            self.index_builder.build_faiss_index(embeddings, chunks)
            self.index_builder.save_index()
            
            self.logger.info("FAISSç´¢å¼•é‡å»ºå®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"é‡å»ºFAISSç´¢å¼•å¤±è´¥: {e}")
            raise
    
    def incremental_update(self, new_pdf_paths: List[str] = None) -> Dict[str, Any]:
        """å¢é‡æ›´æ–°ä¸»å‡½æ•°"""
        start_time = time.time()
        
        try:
            # 1. åŠ è½½ç°æœ‰å‘é‡å’Œchunks
            self.logger.info("=== å¼€å§‹å¢é‡å‘é‡åº“æ›´æ–° ===")
            existing_embeddings, existing_chunks, has_existing = self.load_existing_vectors()
            
            # 2. ç¡®å®šéœ€è¦å¤„ç†çš„æ–°æ–‡ä»¶
            if new_pdf_paths:
                # ä½¿ç”¨æŒ‡å®šçš„PDFæ–‡ä»¶
                new_pdf_files = [path for path in new_pdf_paths if Path(path).exists()]
                if len(new_pdf_files) != len(new_pdf_paths):
                    missing = set(new_pdf_paths) - set(new_pdf_files)
                    self.logger.warning(f"ä»¥ä¸‹æ–‡ä»¶æœªæ‰¾åˆ°: {missing}")
            else:
                # è‡ªåŠ¨å‘ç°æ–°æ–‡ä»¶
                processed_files = self.get_processed_files(existing_chunks) if has_existing else set()
                new_pdf_files = self.find_new_pdf_files(processed_files=processed_files)
            
            if not new_pdf_files:
                self.logger.info("æ²¡æœ‰å‘ç°éœ€è¦å¤„ç†çš„æ–°PDFæ–‡ä»¶")
                return {
                    "status": "success",
                    "message": "æ²¡æœ‰æ–°æ–‡ä»¶éœ€è¦å¤„ç†",
                    "existing_chunks": len(existing_chunks),
                    "new_chunks": 0,
                    "total_chunks": len(existing_chunks),
                    "processing_time": time.time() - start_time
                }
            
            # 3. å¤„ç†æ–°PDFæ–‡ä»¶
            self.logger.info(f"å¼€å§‹å¤„ç† {len(new_pdf_files)} ä¸ªæ–°PDFæ–‡ä»¶...")
            new_chunks = self.process_new_pdfs(new_pdf_files)
            
            if not new_chunks:
                self.logger.warning("æ–°PDFæ–‡ä»¶æœªèƒ½æå–å‡ºä»»ä½•æ–‡æ¡£å—")
                return {
                    "status": "warning",
                    "message": "æ–°PDFæ–‡ä»¶æœªèƒ½æå–å‡ºæ–‡æ¡£å—",
                    "existing_chunks": len(existing_chunks),
                    "new_chunks": 0,
                    "total_chunks": len(existing_chunks),
                    "processing_time": time.time() - start_time
                }
            
            # 4. å‘é‡åŒ–æ–°æ–‡æ¡£å—
            new_embeddings = self.vectorize_new_chunks(new_chunks)
            
            # 5. åˆå¹¶å‘é‡å’Œchunks
            merged_embeddings = self.merge_vectors(existing_embeddings, new_embeddings)
            merged_chunks = self.merge_chunks(existing_chunks, new_chunks)
            
            # 6. æ›´æ–°å‘é‡å­˜å‚¨
            self.update_vector_store(merged_embeddings, merged_chunks)
            
            processing_time = time.time() - start_time
            
            result = {
                "status": "success",
                "message": "å¢é‡æ›´æ–°å®Œæˆ",
                "existing_chunks": len(existing_chunks),
                "new_chunks": len(new_chunks),
                "total_chunks": len(merged_chunks),
                "new_files_processed": [Path(f).name for f in new_pdf_files],
                "processing_time": processing_time
            }
            
            self.logger.info(f"=== å¢é‡æ›´æ–°å®Œæˆ ===")
            self.logger.info(f"å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
            self.logger.info(f"æ–°å¢æ–‡æ¡£å—: {len(new_chunks)}")
            self.logger.info(f"æ€»æ–‡æ¡£å—: {len(merged_chunks)}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"å¢é‡æ›´æ–°å¤±è´¥: {e}")
            raise


class RAGSystem:
    """å¢å¼ºçš„RAGç³»ç»Ÿä¸»ç±»ï¼ˆæ”¯æŒå¢é‡æ›´æ–°ï¼‰"""
    
    def __init__(self, config: RAGConfig):
        self.config = config
        self.logger = self._setup_logging()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.pdf_extractor = StructuredPDFExtractor(config)
        #self.pdf_extractor = PDFExtractor(config)
        self.vectorizer = ChunkVectorizer(config)
        self.index_builder = VectorIndexBuilder(config)
        self.retriever = None
        self.llm_client = LLMClient(config)
        
        # åˆå§‹åŒ–å¢é‡æ›´æ–°å™¨
        self.updater = IncrementalVectorUpdater(config)
        
        # å°è¯•åŠ è½½ç°æœ‰ç´¢å¼•
        if self.index_builder.load_index():
            self.retriever = DocumentRetriever(config, self.index_builder)
            self.vectorizer.load_model()
    
    def _setup_logging(self) -> logging.Logger: 
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(sys.stdout),
                logging.FileHandler('rag_system.log', encoding='utf-8')
            ]
        )
        return logging.getLogger(__name__)
    
    def build_knowledge_base(self, pdf_paths: List[str] = None):
        """æ„å»ºçŸ¥è¯†åº“"""
        self.logger.info("å¼€å§‹æ„å»ºçŸ¥è¯†åº“...")
        
        if pdf_paths is None:
            # æ‰¹é‡å¤„ç†PDFç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
            all_json_chunks = self.pdf_extractor.batch_process_structured_pdfs()
            if all_json_chunks:
                all_chunks = all_json_chunks
            #else:
            #    all_chunks = self.pdf_extractor.batch_process_pdfs()
        else:
        # å¤„ç†æŒ‡å®šçš„PDFæ–‡ä»¶
            all_chunks = []
            for pdf_path in pdf_paths:
                if not Path(pdf_path).exists():
                    self.logger.warning(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
                    continue
                
                try:
                    #if pdf_path.lower().endswith(".json"):
                    all_chunks += self.pdf_extractor.process_structured_pdf_file(pdf_path)
                    #else:
                    #    all_chunks += self.pdf_extractor.process_pdf_with_column_detection(pdf_path)
                except Exception as e:
                    self.logger.error(f"å¤„ç†PDFå¤±è´¥ {pdf_path}: {e}")
        
        if not all_chunks:
            raise ValueError("æ²¡æœ‰æˆåŠŸæå–ä»»ä½•æ–‡æ¡£å—")
        
        # ç”ŸæˆåµŒå…¥
        self.logger.info("å¼€å§‹å‘é‡åŒ–...")
        self.vectorizer.load_model()
        self.vectorizer.load_chunks_from_list(all_chunks)
        
        if not self.vectorizer.vectorize_chunks():
            raise RuntimeError("å‘é‡åŒ–å¤±è´¥")
        
        # ä¿å­˜å‘é‡
        self.vectorizer.save_vectors()
        
        # æ„å»ºç´¢å¼•
        self.logger.info("æ„å»ºå‘é‡ç´¢å¼•...")
        self.index_builder.build_faiss_index(self.vectorizer.embeddings, self.vectorizer.chunks)
        self.index_builder.save_index()
        
        # åˆå§‹åŒ–æ£€ç´¢å™¨
        self.retriever = DocumentRetriever(self.config, self.index_builder)
        
        self.logger.info(f"çŸ¥è¯†åº“æ„å»ºå®Œæˆ,åŒ…å« {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
    
    def add_documents(self, pdf_paths: List[str] = None) -> Dict[str, Any]:
        """æ·»åŠ æ–°æ–‡æ¡£åˆ°å‘é‡åº“"""
        try:
            # æ‰§è¡Œå¢é‡æ›´æ–°
            result = self.updater.incremental_update(pdf_paths)
            
            # é‡æ–°åˆå§‹åŒ–æ£€ç´¢å™¨ä»¥ä½¿ç”¨æ›´æ–°åçš„ç´¢å¼•
            if result["status"] == "success" and result["new_chunks"] > 0:
                if self.index_builder.load_index():
                    self.retriever = DocumentRetriever(self.config, self.index_builder)
                    self.vectorizer.load_model()
                    self.logger.info("æ£€ç´¢å™¨å·²æ›´æ–°ï¼Œå¯ä»¥æŸ¥è¯¢æ–°æ·»åŠ çš„æ–‡æ¡£")
                else:
                    self.logger.warning("ç´¢å¼•åŠ è½½å¤±è´¥ï¼Œæ£€ç´¢å™¨æœªæ›´æ–°")
            
            return result
            
        except Exception as e:
            self.logger.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            raise
    
    def get_document_stats(self) -> Dict[str, Any]:
        """è·å–æ–‡æ¡£åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # åŠ è½½ç°æœ‰æ•°æ®
            existing_embeddings, existing_chunks, has_existing = self.updater.load_existing_vectors()
            
            if not has_existing:
                return {
                    "total_documents": 0,
                    "total_chunks": 0,
                    "processed_files": []
                }
            
            # ç»Ÿè®¡æ–‡ä»¶ä¿¡æ¯
            file_stats = {}
            for chunk in existing_chunks:
                source_file = chunk.get('metadata', {}).get('source_file', 'Unknown')
                if source_file not in file_stats:
                    file_stats[source_file] = {
                        "chunks": 0,
                        "chapters": set()
                    }
                file_stats[source_file]["chunks"] += 1
                chapter = chunk.get('metadata', {}).get('chapter_title', '')
                if chapter:
                    file_stats[source_file]["chapters"].add(chapter)
            
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            processed_files = []
            for filename, stats in file_stats.items():
                processed_files.append({
                    "filename": filename,
                    "chunks": stats["chunks"],
                    "chapters": len(stats["chapters"])
                })
            
            return {
                "total_documents": len(file_stats),
                "total_chunks": len(existing_chunks),
                "embedding_dimension": existing_embeddings.shape[1] if existing_embeddings.size > 0 else 0,
                "processed_files": processed_files
            }
            
        except Exception as e:
            self.logger.error(f"è·å–æ–‡æ¡£ç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e)}
    
    def query(self, question: str, top_k: Optional[int] = None) -> Dict[str, Any]:
        """æŸ¥è¯¢RAGç³»ç»Ÿ"""
        if self.retriever is None:
            raise RuntimeError("æ£€ç´¢å™¨æœªåˆå§‹åŒ–,è¯·å…ˆæ„å»ºçŸ¥è¯†åº“æˆ–æ·»åŠ æ–‡æ¡£")
        
        self.logger.info(f"å¤„ç†æŸ¥è¯¢: {question}")
        start_time = time.time()
        
        try:
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retrieved_docs = self.retriever.retrieve(question, top_k)
            
            retrieval_time = time.time() - start_time
            
            if not retrieved_docs:
                return {
                    "question": question,
                    "answer": "æŠ±æ­‰,æˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                    "sources": [],
                    "retrieval_time": retrieval_time,
                    "generation_time": 0,
                    "total_time": retrieval_time
                }
            
            # æ„å»ºæç¤ºè¯
            context = self._build_context(retrieved_docs)
            full_prompt = self.config.llm.SYSTEM_PROMPT.format(context=context) + f"\n\né—®é¢˜: {question}\n\nå›ç­”:"
            
            # ç”Ÿæˆå›ç­”
            generation_start = time.time()
            messages = [{"role": "user", "content": full_prompt}]
            answer = self.llm_client.generate_response(messages)
            generation_time = time.time() - generation_start
            
            total_time = time.time() - start_time
            
            result = {
                "question": question,
                "answer": answer,
                "sources": [
                    {
                        "chapter": doc["metadata"]["chapter_title"],
                        "page_range": f"{doc['metadata']['start_page']}-{doc['metadata']['end_page']}",
                        "similarity": doc.get("similarity_score", 0),
                        "rerank_score": doc.get("rerank_score", 0),
                        "text_preview": doc["text"][:200] + "..." if len(doc["text"]) > 200 else doc["text"]
                    }
                    for doc in retrieved_docs
                ],
                "retrieval_time": retrieval_time,
                "generation_time": generation_time,
                "total_time": total_time
            }
            
            self.logger.info("æŸ¥è¯¢å¤„ç†å®Œæˆ")
            return result
            
        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            raise
    
    def _build_context(self, documents: List[Dict[str, Any]]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡"""
        if not documents:
            return "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        context_parts = []
        current_length = 0
        
        for i, doc in enumerate(documents, 1):
            doc_info = f"""
æ–‡æ¡£å— {i}:
ç« èŠ‚: {doc['metadata']['chapter_title']}
é¡µç : {doc['metadata']['start_page']}-{doc['metadata']['end_page']}
ç›¸ä¼¼åº¦: {doc.get('similarity_score', 0):.3f}
å†…å®¹: {doc['text']}
---
"""
            
            if current_length + len(doc_info) > self.config.retrieval.MAX_CONTEXT_LENGTH:
                break
                
            context_parts.append(doc_info)
            current_length += len(doc_info)
        
        return "\n".join(context_parts)
    
    def interactive_chat(self):
        """å¢å¼ºçš„äº¤äº’å¼èŠå¤©ç•Œé¢"""
        print("=== å¢å¼ºRAGæ–‡æ¡£é—®ç­”ç³»ç»Ÿ ===")
        print("å¯ç”¨å‘½ä»¤:")
        print("  æ­£å¸¸é—®é¢˜: ç›´æ¥è¾“å…¥é—®é¢˜è¿›è¡ŒæŸ¥è¯¢")
        print("  'add': æ·»åŠ æ–°çš„PDFæ–‡æ¡£")
        print("  'add [è·¯å¾„]': æ·»åŠ æŒ‡å®šè·¯å¾„çš„PDFæ–‡æ¡£")
        print("  'stats': æŸ¥çœ‹æ–‡æ¡£åº“ç»Ÿè®¡ä¿¡æ¯")
        print("  'config': æŸ¥çœ‹å½“å‰é…ç½®")
        print("  'rebuild': é‡æ–°æ„å»ºæ•´ä¸ªçŸ¥è¯†åº“")
        print("  'quit': é€€å‡ºç³»ç»Ÿ")
        print("-" * 50)
        
        while True:
            try:
                user_input = input("\næ‚¨çš„è¾“å…¥: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("æ„Ÿè°¢ä½¿ç”¨!")
                    break
                
                elif user_input.lower().startswith('add'):
                    # å¤„ç†addå‘½ä»¤
                    parts = user_input.split(maxsplit=1)
                    if len(parts) > 1:
                        # æŒ‡å®šè·¯å¾„
                        pdf_path = parts[1].strip()
                        if not Path(pdf_path).exists():
                            print(f"æ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
                            continue
                        result = self.add_documents([pdf_path])
                    else:
                        # è‡ªåŠ¨å‘ç°
                        result = self.add_documents()
                    
                    print(f"æ·»åŠ ç»“æœ: {result['message']}")
                    if result['status'] == 'success':
                        print(f"æ–°å¢æ–‡æ¡£å—: {result['new_chunks']}")
                        print(f"æ€»æ–‡æ¡£å—: {result['total_chunks']}")
                        print(f"å¤„ç†æ—¶é—´: {result['processing_time']:.2f}ç§’")
                        if result.get('new_files_processed'):
                            print(f"å¤„ç†çš„æ–‡ä»¶: {', '.join(result['new_files_processed'])}")
                
                elif user_input.lower() == 'stats':
                    stats = self.get_document_stats()
                    if 'error' in stats:
                        print(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {stats['error']}")
                    else:
                        print(f"ğŸ“Š æ–‡æ¡£åº“ç»Ÿè®¡:")
                        print(f"  æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
                        print(f"  æ€»æ–‡æ¡£å—: {stats['total_chunks']}")
                        print(f"  å‘é‡ç»´åº¦: {stats['embedding_dimension']}")
                        print(f"  å·²å¤„ç†æ–‡ä»¶:")
                        for file_info in stats['processed_files'][:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                            print(f"    ğŸ“„ {file_info['filename']}: {file_info['chunks']} å—, {file_info['chapters']} ç« èŠ‚")
                        if len(stats['processed_files']) > 10:
                            print(f"    ... è¿˜æœ‰ {len(stats['processed_files']) - 10} ä¸ªæ–‡ä»¶")
                
                elif user_input.lower() == 'config':
                    print(f"âš™ï¸ å½“å‰é…ç½®:")
                    print(f"  LLMç±»å‹: {self.config.llm.LLM_TYPE}")
                    print(f"  LLMæ¨¡å‹: {self.config.llm.MODEL_NAME}")
                    print(f"  åµŒå…¥æ¨¡å‹: {self.config.embedding.EMBEDDING_MODEL}")
                    print(f"  åµŒå…¥æä¾›å•†: {self.config.embedding.EMBEDDING_PROVIDER}")
                    print(f"  æ£€ç´¢æ•°é‡: {self.config.retrieval.FINAL_TOP_K}")
                    print(f"  ç›¸ä¼¼åº¦é˜ˆå€¼: {self.config.retrieval.SIMILARITY_THRESHOLD}")
                    print(f"  PDFç›®å½•: {self.config.PDF_DIR}")
                
                elif user_input.lower() == 'rebuild':
                    confirm = input("âš ï¸ è¿™å°†é‡æ–°æ„å»ºæ•´ä¸ªçŸ¥è¯†åº“ï¼Œç»§ç»­å—? (y/N): ").strip().lower()
                    if confirm == 'y':
                        print("ğŸ”„ å¼€å§‹é‡æ–°æ„å»ºçŸ¥è¯†åº“...")
                        self.build_knowledge_base()
                        print("âœ… çŸ¥è¯†åº“é‡æ–°æ„å»ºå®Œæˆ!")
                    else:
                        print("æ“ä½œå·²å–æ¶ˆ")
                
                elif user_input:
                    # æ­£å¸¸æŸ¥è¯¢
                    if not self.retriever:
                        print("âŒ æ£€ç´¢å™¨æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆæ·»åŠ æ–‡æ¡£æˆ–æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æœ‰å‘é‡åº“")
                        print("   å¯ä»¥ä½¿ç”¨ 'add' å‘½ä»¤æ·»åŠ æ–‡æ¡£ï¼Œæˆ– 'rebuild' é‡æ–°æ„å»ºçŸ¥è¯†åº“")
                        continue
                    
                    result = self.query(user_input)
                    
                    print(f"\nğŸ¤– å›ç­”: {result['answer']}")
                    print(f"\nâ±ï¸ ç”¨æ—¶: æ£€ç´¢ {result['retrieval_time']:.2f}s | ç”Ÿæˆ {result['generation_time']:.2f}s | æ€»è®¡ {result['total_time']:.2f}s")
                    
                    #if result.get("sources"):
                    #    print(f"\nğŸ“š å‚è€ƒæ¥æº ({len(result['sources'])} ä¸ª):")
                    #    for i, source in enumerate(result["sources"], 1):
                    #        print(f"  {i}. {source['chapter']} (é¡µ{source['page_range']}) - ç›¸ä¼¼åº¦: {source['similarity']:.3f}")
                    #        print(f"     é¢„è§ˆ: {source['text_preview']}")
                
            except KeyboardInterrupt:
                print("\n\næ„Ÿè°¢ä½¿ç”¨!")
                break
            except Exception as e:
                print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")
                self.logger.error(f"äº¤äº’å¼èŠå¤©é”™è¯¯: {e}")


# ================================
# ä½¿ç”¨ç¤ºä¾‹å’Œé…ç½®
# ================================

def get_openai_config() -> RAGConfig:
    """OpenAIé…ç½®ç¤ºä¾‹"""
    config = RAGConfig()
    config.llm.LLM_TYPE = "openai"
    config.llm.MODEL_NAME = "deepseek-r1-0528"
    config.llm.API_KEY = ""
    config.llm.API_BASE = ''
    config.llm.TEMPERATURE = 0.1
    return config

#def get_ollama_config() -> RAGConfig:
#    """Ollamaæœ¬åœ°é…ç½®ç¤ºä¾‹"""
#    config = RAGConfig()
#    config.llm.LLM_TYPE = "ollama"
#    config.llm.MODEL_NAME = "llama2"
#    config.llm.API_BASE = "http://localhost:11434"
#    config.llm.TEMPERATURE = 0.1
#    return config

def main():
    """ä¸»å‡½æ•°"""
    print("=== å¢å¼ºPDFæ–‡æ¡£RAGç³»ç»Ÿ ===")
    #print("é€‰æ‹©LLMåç«¯:")
    #print("1. OpenAI API (éœ€è¦API key)")
    #print("2. Ollama (æœ¬åœ°æ¨¡å‹)")
    
    #choice = input("è¯·é€‰æ‹© (1-2): ").strip()
    
    #if choice == "1":
    config = get_openai_config()
    if not config.llm.API_KEY:
        api_key = input("è¯·è¾“å…¥OpenAI API Key: ").strip()
        if api_key:
            config.llm.API_KEY = api_key
    #elif choice == "2":
    #    config = get_ollama_config()
    #    model = input("è¯·è¾“å…¥Ollamaæ¨¡å‹åç§° (é»˜è®¤: llama2): ").strip()
    #    if model:
    #        config.llm.MODEL_NAME = model
    #else:
    #    print("æ— æ•ˆé€‰æ‹©,ä½¿ç”¨é»˜è®¤OpenAIé…ç½®")
    #    config = get_openai_config()
    
    try:
        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        rag = RAGSystem(config)
        
        # æ£€æŸ¥ç°æœ‰å‘é‡åº“çŠ¶æ€
        stats = rag.get_document_stats()
        if stats.get('total_chunks', 0) > 0:
            print(f"\nâœ… å‘ç°ç°æœ‰å‘é‡åº“:")
            print(f"   ğŸ“„ {stats['total_documents']} ä¸ªæ–‡æ¡£")
            print(f"   ğŸ“¦ {stats['total_chunks']} ä¸ªæ–‡æ¡£å—")
            print(f"   ğŸ“ {stats['embedding_dimension']} ç»´å‘é‡")
        else:
            print("\nğŸ†• æœªå‘ç°ç°æœ‰å‘é‡åº“")
            
            # æ£€æŸ¥æ–‡ä»¶ç›®å½•
            doc_dir = Path(config.PDF_DIR)
            if not doc_dir.exists():
                doc_dir.mkdir(parents=True, exist_ok=True)
                print(f"ğŸ“ å·²åˆ›å»ºPDFç›®å½•: {doc_dir}")
            
            doc_files = list(doc_dir.glob("*.json"))
            if doc_files:
                print(f"ğŸ“‹ å‘ç° {len(doc_files)} ä¸ªjsonæ–‡ä»¶")
                build_choice = input("æ˜¯å¦ç«‹å³æ„å»ºçŸ¥è¯†åº“? (Y/n): ").strip().lower()
                if build_choice != 'n':
                    print("ğŸ”„ å¼€å§‹æ„å»ºçŸ¥è¯†åº“...")
                    rag.build_knowledge_base()
                    print("âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆ!")
            else:
                print(f"ğŸ“ PDFç›®å½• {doc_files} ä¸­æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶")
                print("ğŸ’¡ å¯ä»¥ä½¿ç”¨ 'add [æ–‡ä»¶è·¯å¾„]' å‘½ä»¤æ·»åŠ æ–‡æ¡£")
        
        # å¯åŠ¨äº¤äº’å¼èŠå¤©
        rag.interactive_chat()
        
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–RAGç³»ç»Ÿå¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥:")
        print("1. PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨")
        print("2. LLMé…ç½®æ˜¯å¦æ­£ç¡®") 
        print("3. ç›¸å…³ä¾èµ–æ˜¯å¦å·²å®‰è£…")

if __name__ == "__main__":
    main()